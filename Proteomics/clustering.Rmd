Clustering methods
========================================================

Methods for clustering (finding groups inside data) are multiple. They often need to know in advance the number of groups in which data have to be partitioned. In that case, they are called supervised clustering methods (e.g. k-means, spectral clustering). 

This domain is not specific to proteomics, but is widely applied to proteomics results, especially in protein quantitation.


Hierarchical clustering
--------------------------------------------------------------------------------

Hierarchical clustering is an unsupervised method, in the sense that it doesn't need a priori to know how many groups are in the data. It just creates a tree based on how close each element is from another. It's an agglomerative approach, as it iteratively puts together the closest elements.
A given tree obtained by hierarchival clustering can be subsequently cut in order to obtain a desired number of groups.


```{r}
## For this chapter, we will use the Iris dataset
## We will cluster the values, and see if they cluster according to their species
data(iris)
irisValues <- iris[, c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")]
```


### Computing distances

Before any clustering, i.e. before any grouping of the data, one has to compute distances between the elements to be clustered. This is usually achieved by the dist() function.
Default method uses the euclidean distance. For data sets of discrete values, the "manhattan" distance can be used.

```{r}
distances <- as.matrix(dist(irisValues, method="euclidean"))
```


### Hierarchical clustering

From the distances, we can compute a hierarchy.

```{r}
hclustTree <- hclust(as.dist(distances))
```


### Plotting dendrograms from distances

The object returned by hclust() can directly be plotted using the base graphics package.

```{r}
plot(hclustTree)
```

Alternatively, the object of class "hclust" returned by a call to hclust() can be turned into a dendrogram. Plotting dendrograms offer more possibilities and accessible parameters.

```{r}
## Changing the orientation of the dendrogram, and the size of the labels
dendrogram1 <- as.dendrogram(hclustTree)
plot(dendrogram1, nodePar=list(lab.cex=0.7, cex=NA), horiz = TRUE)
```

```{r}
## A more complex example using dendrapply to change leaves color and label size
## Color is now function of the Species
colorLabels <- factor(iris$Species)

dendrogramH <- as.dendrogram(hclustTree)
dendColored <- dendrapply(dendrogramH, FUN=function(n) {
    if(is.leaf(n)) {
        att <- attributes(n)
        attr(n, "nodePar") <- c(
            att$nodePar,
            list(lab.col = as.numeric(colorLabels[as.numeric(att$label)]), 
                 lab.cex=.7, cex=NA)
            )
    }
    return(n)
})
plot(dendColored)

```

### Getting k clusters

A tree can be cut in order to get a given number of groups.

```{r}
threeGroups <- cutree(hclustTree, k=3)
```

This partitioning can also be plotted.

```{r}
## Draw dendogram 
plot(hclustTree) 
## Draw red borders around the k clusters
rect.hclust(hclustTree, k=3, border="red") 
```

We can compare this partitioning with the species of each observation.

```{r, echo=FALSE}
library("ggplot2")
qplot(data=iris, x=Sepal.Length, y=Sepal.Width, colour=Species, main="Sepal data, coloured by Species")
qplot(data=iris, x=Sepal.Length, y=Sepal.Width, colour=factor(threeGroups), main="Sepal data, coloured by hclust")
qplot(data=iris, x=Petal.Length, y=Petal.Width, colour=Species, main="Petal data, coloured by Species")
qplot(data=iris, x=Petal.Length, y=Petal.Width, colour=factor(threeGroups), main="Petal data, coloured by hclust")
```

Knowing that a tree also has a height, one can cut the tree according to a desired height, or get all groups corresponding to a set of height.

```{r}
woodSlices <- cutree(hclustTree, h=1:floor(max(hclustTree$height)))
## Display the groups of the first 20 observations.
woodSlices[1:20, ]
```

--------------------------------------------------------------------------------

K-means clustering
--------------------------------------------------------------------------------

This is a supervised clustering, as one has to specify a number of groups before performing the clustering. It is also called a partitioning approach.
The method needs 

The algorithm can be described as follow:
- Assign elements to a given number of clusters
- Compute centroids of each cluster
- Re-assign elements to cluster with closest centroid
- Re-calculate centroids...
- Comtinue until convergence (no re-assignment)

This method may give a non-stable solution ("did not converge in X iterations"). In that case (and in general) use the 'nstart' parameter for it to get a consensus partitioning from 'nstart' runs, or increase 'iter.max', the number of iterations.

```{r}
## Set clusters number 
nbClusters4kmeans <- 3
## Maximal number of iterations
nbMaxIteration <- 100
## Algorithm to be used
algo <- "Hartigan-Wong" # or "Lloyd", "Forgy", "MacQueen"

# Compute clustering
kmeansClust <- kmeans(irisValues, 
                      centers=nbClusters4kmeans, 
                      iter.max=nbMaxIteration,
                      nstart=100,
                      algorithm=algo)
kmeansThreeGroups <- kmeansClust$cluster
```

We can also compare the clustering obtained by this method with the real clustering by species.

```{r, echo=FALSE}
library("ggplot2")
qplot(data=iris, x=Sepal.Length, y=Sepal.Width, 
      colour=Species, main="Sepal data, coloured by Species")
qplot(data=iris, x=Sepal.Length, y=Sepal.Width, 
      colour=factor(kmeansThreeGroups), main="Sepal data, coloured by k-means")
qplot(data=iris, x=Petal.Length, y=Petal.Width, 
      colour=Species, main="Petal data, coloured by Species")
qplot(data=iris, x=Petal.Length, y=Petal.Width, 
      colour=factor(kmeansThreeGroups), main="Petal data, coloured by k-means")
```

Other methods are available in the 'cluster' package (pam).

-------------------------------------------------------------------------------- 

Evidence Accumulation Clustering
-------------------------------------------------------------------------------- 

An interesting method, called Evidence Accumulation Clustering, is described [here](http://things-about-r.tumblr.com/post/43894016034/the-wisdom-of-crowds-clustering-using-evidence). It is based on computing multiple partitions and grouping in the final partitioning elements often co-occuring together in the same clusters.

From the results shown here this method looks interesting but mainly to find well-individualized groups.

> Fred, A. L., & Jain, A. K. (2002). Data clustering using evidence accumulation. In Pattern Recognition, 2002. Proceedings. 16th International Conference on (Vol. 4, pp. 276-280). IEEE.
> 
More information [here](http://www.cs.msu.edu/prip/ResearchProjects/cluster_research/papers/AFred_AJain_ICPR2002.pdf) and [there](http://web.cse.msu.edu/prip/Files/AFred_AJain_SSPR2002.pdf)


The following code is provided on the blog (first link).

```{r}
createCoAssocMatrix <- function(Iter, rangeK, dataSet) {
    nV <- dim(dataSet)[1]
    CoAssoc <- matrix(rep(0, nV * nV), nrow = nV)

    for (j in 1:Iter) {
        jK <- sample(c(rangeK[1]:rangeK[2]), 1, replace = FALSE)
        jSpecCl <- kmeans(x = dataSet, centers = jK)$cluster
        CoAssoc_j <- matrix(rep(0, nV * nV), nrow = nV)
        for (i in unique(jSpecCl)) {
            indVenues <- which(jSpecCl == i)
            CoAssoc_j[indVenues, indVenues] <- CoAssoc_j[indVenues, indVenues] + (1/Iter)
        }
        CoAssoc <- CoAssoc + CoAssoc_j
    }
    return(CoAssoc)
}

eac <- function(Iter, rangeK, dataset, hcMethod = "single") {
    CoAssocSim <- createCoAssocMatrix(Iter, rangeK, dataset)

    # transform from similiarity into distance matrix
    CoAssocDist <- 1 - CoAssocSim
    hclustM <- hclust(as.dist(CoAssocDist), method = hcMethod)
    # determine the cut
    cutValue <- hclustM$height[which.max(diff(hclustM$height))]  
    return(cutree(hclustM, h = cutValue))
}
```

We can use the method on the Iris data set.
```{r}
set.seed(1234)
testClusterMeth <- eac(Iter = 100, rangeK = c(2, 25), dataset = irisValues, hcMethod = "single")
table(testClusterMeth)

```

```{r, echo=FALSE}
library("ggplot2")
qplot(data=iris, x=Sepal.Length, y=Sepal.Width, 
      colour=Species, main="Sepal data, coloured by Species")
qplot(data=iris, x=Sepal.Length, y=Sepal.Width, 
      colour=factor(testClusterMeth), main="Sepal data, coloured by EAC")
qplot(data=iris, x=Petal.Length, y=Petal.Width, 
      colour=Species, main="Petal data, coloured by Species")
qplot(data=iris, x=Petal.Length, y=Petal.Width, 
      colour=factor(testClusterMeth), main="Petal data, coloured by EAC")
```


--------------------------------------------------------------------------------

Problem of supervised clustering
--------------------------------------------------------------------------------

The main problem with supervised clustering, or clustering data in k groups, is that in discovery mode there is no a priori knowledge of the real number of groups. Moreover, each clustering method giving a different clustering of the data it is very difficult to decide whether a given partitioning is relevant or not.

One approach is to use a metric/score to evaluate clustering outputs. There exists a set of well known criteria, like the “Dunn-Index”, the “Calinski-Harabasz-Index”, the "Silhouette measure". 

In the case of k-means clustering, a simplistic approach is to minimize the total distance between the elements of each cluster.

```{r}
## Maximal number of iterations
nbMaxIteration <- 100
## Algorithm to be used
algo <- "Hartigan-Wong" # or "Lloyd", "Forgy", "MacQueen"

distWithins <- vapply(2:25, FUN.VALUE=c(Within=0),
                     FUN=function(nbClusters) {
                         return(
                             kmeans(irisValues, 
                                    centers = nbClusters, 
                                    iter.max = 100, 
                                    algorithm = "Hartigan-Wong")$tot.withinss)
                     })
plot(x=2:25, y=distWithins, pch=16, type="b")
```


Other scores are available in the clusterCrit package.



--------------------------------------------------------------------------------

Fuzzy clustering (c-means)
--------------------------------------------------------------------------------

```{r, echo=FALSE}
suppressPackageStartupMessages(library('Mfuzz'))
```


```{r, echo=FALSE}
data(yeast)

# summary(exprs(yeast))

## Missing values
## Filter
yeast.r <- filter.NA(yeast, thres=0.25)
## Fill
yeast.f <- fill.NA(yeast.r, mode="mean")

## Filtering
tmp <- filter.std(yeast.f,min.std=0)

## Standardization
yeast.s <- standardise(yeast.f)

## Clustering
cl <- mfuzz(eset=yeast.s, centers=16, m=1.25)
mfuzz.plot(yeast.s, cl=cl, mfrow=c(4,4), time.labels=seq(0,160,10))
```

```{r, echo=FALSE}
suppressPackageStartupMessages(library(fda))
data(growth)
growthDataGirls <- growth$hgtf[, order(apply(growth$hgtf,2,max))]
growthDataGirls <- growthDataGirls[,1:39]
growthDataBoys <- growth$hgtm[, order(apply(growth$hgtm,2,max))]
growthDataBoys <- growthDataBoys[,1:39]
growthDataGirlsBoys <- t(cbind(growthDataGirls, growthDataBoys))
whichSex <- ifelse(grepl(x=rownames(growthDataGirlsBoys), pattern="girl"), "Girl", "Boy")

expSet <- new("ExpressionSet", exprs=growthDataGirlsBoys)
growth.r <- filter.NA(expSet, thres=0.25)
growth.s <- standardise(growth.r)

cl <- mfuzz(eset=growth.s, centers=2, m=1.25)
mfuzz.plot(growth.r, cl=cl, mfrow=c(1,2))
fuzzyClusters <- cl$cluster
fuzzyClusters[apply(cl$membership,1,max)<0.99] <- 0
table(whichSex, fuzzyClusters)

```


```{r, echo=FALSE}
load(paste0(getwd(), "/../data/medfly.Rdata"))
medflyData <- t(medfly$eggcount)

expSet <- new("ExpressionSet", exprs=medflyData)
medfly.s <- standardise(expSet)
cl <- mfuzz(eset=medfly.s, centers=10, m=1.25)
fuzzyClusters <- cl$cluster
fuzzyClusters[apply(cl$membership,1,max)<0.7] <- 0


### Bases fit 4-order
medflyData <- medfly$eggcount
timePoints <- 13:38
knots <- seq(13, 38,length.out=7)
bspline4 <- create.bspline.basis(norder=4, breaks=knots)

layout(matrix(1:8, nrow=2, byrow=TRUE))

aMedflyCluster <- medflyData[,fuzzyClusters==1]
fit2data4clust <- Data2fd(argvals=timePoints, y=aMedflyCluster, basisobj=bspline4)
plot(fit2data4clust, xlab="age", ylab="Nb. of eggs laid",
         main="Mediterranean Fruit Fly 1", sub='Fit of the 4-order bases', 
         ylim=c(0,max(medflyData)))

aMedflyCluster <- medflyData[,fuzzyClusters==2]
fit2data4clust <- Data2fd(argvals=timePoints, y=aMedflyCluster, basisobj=bspline4)
plot(fit2data4clust, xlab="age", ylab="Nb. of eggs laid",
         main="Mediterranean Fruit Fly 2", sub='Fit of the 4-order bases', 
         ylim=c(0,max(medflyData)))

aMedflyCluster <- medflyData[,fuzzyClusters==3]
fit2data4clust <- Data2fd(argvals=timePoints, y=aMedflyCluster, basisobj=bspline4)
plot(fit2data4clust, xlab="age", ylab="Nb. of eggs laid",
         main="Mediterranean Fruit Fly 3", sub='Fit of the 4-order bases', 
         ylim=c(0,max(medflyData)))

aMedflyCluster <- medflyData[,fuzzyClusters==4]
fit2data4clust <- Data2fd(argvals=timePoints, y=aMedflyCluster, basisobj=bspline4)
plot(fit2data4clust, xlab="age", ylab="Nb. of eggs laid",
         main="Mediterranean Fruit Fly 4", sub='Fit of the 4-order bases', 
         ylim=c(0,max(medflyData)))

aMedflyCluster <- medflyData[,fuzzyClusters==5]
fit2data4clust <- Data2fd(argvals=timePoints, y=aMedflyCluster, basisobj=bspline4)
plot(fit2data4clust, xlab="age", ylab="Nb. of eggs laid",
         main="Mediterranean Fruit Fly 5", sub='Fit of the 4-order bases', 
         ylim=c(0,max(medflyData)))

aMedflyCluster <- medflyData[,fuzzyClusters==6]
fit2data4clust <- Data2fd(argvals=timePoints, y=aMedflyCluster, basisobj=bspline4)
plot(fit2data4clust, xlab="age", ylab="Nb. of eggs laid",
         main="Mediterranean Fruit Fly 6", sub='Fit of the 4-order bases', 
         ylim=c(0,max(medflyData)))

aMedflyCluster <- medflyData[,fuzzyClusters==7]
fit2data4clust <- Data2fd(argvals=timePoints, y=aMedflyCluster, basisobj=bspline4)
plot(fit2data4clust, xlab="age", ylab="Nb. of eggs laid",
         main="Mediterranean Fruit Fly 7", sub='Fit of the 4-order bases', 
         ylim=c(0,max(medflyData)))

aMedflyCluster <- medflyData[,fuzzyClusters==0]
fit2data4clust <- Data2fd(argvals=timePoints, y=aMedflyCluster, basisobj=bspline4)
plot(fit2data4clust, xlab="age", ylab="Nb. of eggs laid",
         main="Mediterranean Fruit Fly 0", sub='Fit of the 4-order bases', 
         ylim=c(0,max(medflyData)))


mfuzz.plot(expSet, cl=cl, mfrow=c(2,5),min.mem=0.7)

```




<a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/3.0/80x15.png" /></a><br />This work by <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName">Celine Hernandez</span> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/deed.en_US">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.